---
title: "Minnesota Emergency SNAP Data Story"
author: "Ben Jaques-Leslie"
date: today
date-format: long
institute: "Minnesota Department of Human Services"
format: 
   revealjs:
     theme: [style.scss]
     show-notes: false #true separate-page
     slide-number: true
     incremental: true 
     logo: "DHS Logo RGB (Reverse)_6w.png"
     footer: <https://mn.gov/dhs/>
     smaller: false
params:
  day_start:  '2020-03-01'
  day_end: '2023-10-01'
---

```{r set up}
# Load two packages
library(dplyr)
library(stringr)
library(odbc)
library(tidyverse)
library(ggthemes)
library(MNColorrrs)
library(epoxy)

con <- dbConnect(odbc::odbc(),
                 "Terapass",
                 timeout = Inf,
                 UID = keyring::key_list("data-warehouse")[1,2],
                 PWD = keyring::key_get("data-warehouse", 
                                        keyring::key_list("data-warehouse")[1,2]),
                 bigint = "numeric")

gg_style = function(in_chart)
{
  
  # theme_fivethirtyeight() +
  # scale_y_continuous(labels = scales::comma) +
  theme(
    text = element_text(size = 10),
    legend.title = element_blank(),
    legend.position = "bottom",
    legend.direction = "horizontal",
    axis.text.x = element_text(angle = 0),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    legend.background = element_rect(fill = "white"),
    strip.background = element_rect(fill = "white"),
    legend.key = element_rect(fill = "white"),
    strip.text.y = element_text(angle = 0),
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )
}
```

## TLDR

Data-centric view of Minnesota's Emergency SNAP program

. . .

Case for using data science in government

::: notes
Big picture this is my data-centric perspective of Minnesota's Emergency SNAP program. There's tons that I don't know about the program from policy, operations, and IT, but data played a major role in getting and keeping the program running.

I also see our experience as a case for using data science in government. As I think you'll see, using data science principles was key to getting the program up and running.

With that let's go back to March 2020.
:::

## March 2020

[![](families-first.png){fig-alt="Families First Coronavirus Response Act text about ESNAP"}](https://www.congress.gov/116/plaws/publ127/PLAW-116publ127.pdf)

::: notes
I don't need to remind you what was happening then. I had two kids under 5 running around the house, while my wife and I tried to work and keep everyone safe and happy. Congress passed the Families First Coronavirus Response Act, which included this section, creating Emergency SNAP.
:::

## Emergency SNAP

Provide to all participants in SNAP the maximum allotment for their case size, regardless of income.

::: notes
What Emergency SNAP does is simple. Provide maximum food allotments to all eligible case regarless of income. To me it sounded like an IT problem. I knew that there wasn't a button in our mainframe eligibility/issuance system to provide maximum benefits, but I figured Minnesota's IT would be in charge.
:::

## MNIT, can you help?

Yes!

. . .

and no.

. . .

We can't issue the benefits, but...

. . .

we can write a script that will issue benefits to common cases.

::: notes
MN SNAP team brought Minnesota's IT, called MNIT, into the discussion, asking if they could help. There answer was yes... and no... They couldn't issue the benefits. That didn't sound too good. But they could write a script to help get food assistance issued. This was a big deal, because you could image that workers would need to manually issue benefits, which would have been tons of work.
:::

## Types of cases

| Cases           | SNAP | MFIP |
|-----------------|------|------|
| Federal funding | Yes  | Yes  |
| State funding   | Yes  | Yes  |
| Mixed funding   | Yes  | Yes  |
| Cash-out        | Yes  | No   |

::: notes
Now I said that MNIT would be able to create a script to issue to common cases. What do I mean by that? In Minnesota, there are two principle programs that issue SNAP food support: SNAP and MFIP, the Minnesota Family Investment Program, which is our combined SNAP and TANF program. Now, you can categorize cases in dozens of ways, but for Emergency SNAP there are 7 types to keep in mind, mostly divided by funding. We have federally-funded cases, state-funded cases, mixed cases that receive both state and federal funds, and cash-out cases that receive food benefits as cash. MNIT's scripts would issue to all federal and state funded cases, but not the others. There were thousands of cases missing. How was the SNAP team going to do this?
:::

## Data team, can you help?

Yes!

. . .

Not sure how, yet...

. . .

but yes!

::: notes
So the SNAP team came to us in the data team to help. Now our normal work is reports, data request, performance measures, the occasional research project. Not helping with issuance. So this was a new request to us, but we were in uncharted territory.  We said yes, we will help. But we really didn't know how to do what they wanted. But we felt pretty sure that we could figure it out. 
:::

## Exact request

-   Spreadsheets of common cases with **no more than 500 rows and two columns**

    -   Casenumber

    -   Amount to maximum benefit

-   Spreadsheets of other cases with more information

::: notes
Now turning to the more particular ask. To issue benefits through MNIT's scripts, the SNAP team needed speadsheets in a particular format. They could only be two columns (Casenumbers and the amount to max benefits) and could be no more than 500 rows.

For less common cases, the SNAP team wanted spreadsheets that included the amounts for the different funding as well as additional data.

Common cases were federal and state funded.

Later on we would add new lists for stale dated warrents and participants who had passed away
:::

## Minnesota food assistance cases

```{r}
con %>%
  tbl(sql(
    glue::glue(
      "SELECT
      a.ProgramID
      , count(DISTINCT a.CaseNumber) as cases
FROM MaxisViews.CAFSApprovalV a --
WHERE a.BenefitMonth < Add_Months(DATE'2020-04-01' , +1) -- will add one month to the date you enter at the prompt -- Enter month as YYYY-MM-DD (day always 01)
    AND a.BenefitMonth >= Add_Months(DATE'2020-04-01' , -25) -- benefit month occurred less than 2 years plus one month before report month
    AND a.DerivedEndDate > DATE'2020-04-01' -- prompt date
    AND a.ProgramStatCode = 'A'
    AND a.ResultCode <> 'I'
    AND a.ProgramID in ('MF','FS') --MFIP is an example. Enter program code (i.e., DW, FS,MF, RC) here.
    --AND a.issueongoingamount >0 -- include this qualifier if you only want cases eligible for $ in that month.
group by a.ProgramID"
    )
  )) %>% 
  collect() %>% 
  mutate(ProgramID = fct_reorder(ProgramID,cases),
         ProgramID = fct_recode(ProgramID,
                                 "Minnesota Family Investment Program" = "MF",
                                 "Supplemental Nutrition Assistance Program" = "FS")) %>% 
  ggplot(aes(x = "", y = cases, fill = ProgramID)) +
  geom_col(stat = "identity") +
  geom_text(aes(label = scales::comma(cases), group = ProgramID), color = "white",
                  position = position_stack(vjust = .5)) +
  scale_fill_mn_state() +
  scale_y_continuous(labels = scales::comma) +
  theme_fivethirtyeight() +
  labs(
    title = "Cases in April 2020"
  ) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  )
```

::: notes
Now in April 2020, Minnesota had more than 200000 cases receiving food support. The request above was going to be lots and lots of spreadsheets to issue benefits to these families. And even though MNIT couldn't issue the benefits directly, having a script to do it was going to be a huge help.
:::

## RStudio

::: columns
::: {.column width="40%"}
-   Code

    -   SQL

    -   R

-   Scripts

    -   .Rmd

    -   .R
:::

::: {.column width="60%"}
-   Outputs

    -   Excel

    -   Html reports
:::
:::

::: notes
So I turned to RStudio and began to work. I wrote in SQL to extract data from our data warehouse. This SQL was wrapped in R functions that allowed me to use them in different extractions. Beyond functions, I used R to wrangle the data, attach different data sets, review the results, and save the spreadsheets. I wrote in two files types: .R (mainly for functions) and .Rmd. The .Rmd files were the core part of the work that produced both html reports and the the spreadsheets.
:::

## What does the core script do?

1.  Collect eligibility and budget information

2.  Calculate food amount

3.  Create table of maximum SNAP allotments

4.  Join maximum allotments to cases

5.  Calculate emergency SNAP amount

6.  Save data in data warehouse

7.  Produce spreadsheets

::: notes
The core script extracts the eligibility and budget information for cases in the previous month. Off of this I calculated the food amount the case received. This may seem trivial, but because of the different programs (SNAP, MFIP) and the ways that the benefit could be funded, it could get very complicated. Next I created a table of maximum SNAP allotments. This table is generally available, but not structured in a useable way. I needed to added rows for larger families for example. Then I joined the table of maximum allottments to the case-level data by the number of eligible members in the household. With that I was able to calculate the extra issuance to reach the maximum. In many cases, this was simply subtracting the issued food support from the maximum allotment. For cases with mixed funding the calculation was more complicated. Next I saved the data in our data warehouse. Finally, I produced spreadsheets with MNIT's specifications for common cases and other sheets for less common cases.

These scripts were written in Rmarkdown, which braids together text and code, in this case R and SQL. There was a separate scripts for SNAP and MFIP. In addition to extracting the data, performing operations, and save the spreadsheets, the files also produced HTML reports of the process so that I had a record of data extraction and process.

I'll note that I wrote R scripts for the functions that extracted the data to make sure they remained consistent across different scripts.

Along the way the script checks the data

Separate scripts for SNAP and MFIP, because the budget data was stored differently

Similar scripts for retroactively eligible scripts

Different calculations for federal, state, and mixed cases

Removed online purchasing cases

Removed other OIG and FNS cases

Remove intercepted cases
:::

## Retroactive eligibility

-   Needed to issue emergency payments to cases found to be retroactively eligible

-   How did we do that:

    -   Same steps as core scripts

    -   Remove cases that were already issued benefits

::: notes
I've already mentioned retroactive eligibility. We wanted to make sure that cases that were found to be retroactively eligible would receive emergency SNAP for past months. To do this, I adapted the core scripts, removing cases cases that were already issued benefits to identify newly eligible cases to issue new benefits too.
:::

## Monthly process

1.  Execute core scripts for previous month of eligible participants

2.  Create table of previous months of ESNAP

3.  Iterate previous months over retroactive scripts

4.  Execute other scripts

5.  SNAP team gets the benefits issued

    1.  Execute MNIT script for common cases

    2.  Manually issue benefits for less common cases

::: notes
How did we do this each month? Each month we'd execute the core scripts for MFIP and SNAP to determine ESNAP payments for participants in the previous month. Then I'd create a table of all the months from the beginning of ESNAP (March 2020) until the month before the previous month. Then I would iterated these months over the scripts for retroactive cases creating issuance spreadsheets for each month before. I'd execute other scripts that were added over time. Once all of this was done, I'd have dozens of reports and even more spreadsheets. I'd pass the spreadsheets along to the SNAP team who would do the work of issuing the benefits either through the MNIT scripts or with much great work manually for less common cases.

The process up until the spreadsheets were sent to the SNAP team, was all done with an R script. The script execute the core Rmarkdown scripts and additional scripts. For the retroactive scripts, the R script iterated their execution using purrr::map.

For me, all I did was execute the R script and all the files executed.

This goes along for about a year...
:::

## April 2021

. . .

\$95 minimum payment

. . .

Hey!

. . .

It's MNIT!

. . .

Remember how you wanted to issue benefits directly?

. . .

We can do that now! But...

. . .

**For common cases**

. . .

And not for retroactive cases

::: notes
In April 2021, we needed to make some changes. In particular, a minimum payment of \$95 was added, which fundamentally changed the calculations of the Emergency SNAP issuance.

In addition, MNIT returned with some solutions for automated issuance! They could now automate the process, but it still was just for common cases and not for retroactive cases. Honestly, I didn't event know that they were working on it. I was happy for my SNAP team colleagues, because it would save them a lot of work. For me, it meant a considerable reworking of the core scripts.
:::

## Adjusting the core scripts

1.  Collect issuance data from MNIT
2.  Compare to cases extracted from the data warehouse
3.  Remove cases that were issued by MNIT
4.  Calculate Emergency SNAP benefit with a \$95 minimum

::: notes
Now the core scripts need to collect issuance data from MNIT that was accomplished through automation. This data was structured as .csv and .txt files, which needed to be handled differently from data coming from the data warehouse. After loading this data, I extracted the case data from the data warehouse and compared it to the cases issued through automation and removed cases that that had already received the issuance.

I then needed to rewrite the calculations for the Emergency SNAP payment, keeping the \$95 minimum in mind. For cases that received federal or state funding only, it was fairly easy to code, but mixed cases were very complicated.

Again this was all accomplished in R through the core Rmarkdown files.
:::

# Inputs and outputs

```{r}
# Files in scripts
files_scripts <- list.files(path = "S:/D068/TES_Research/Data Requests/REQ_1100/1197 COVID-19_SNAP_Emergency_Issuance/scripts/", recursive = T, full.names = T)

# Files in output
files_output <- list.files(path = "S:/D068/TES_Research/Data Requests/REQ_1100/1197 COVID-19_SNAP_Emergency_Issuance/output/", recursive = T, full.names = T)

# .R files
files_r <- 
  files_scripts %>% 
  str_subset("[.][R]$")

# .Rmd files
files_rmd <- 
  files_scripts[grepl('.*\\.Rmd$', files_scripts)]

# .html files
files_html <- 
  files_output[grepl('.*\\.html$', files_output)]

# .html files
files_xlsx <- 
  files_output[grepl('.*\\.xlsx$', files_output)]

prep_c_01 <-
  c(length(files_xlsx),
    length(files_html),
    length(files_r),
    length(files_rmd))

prep_c_02 <-
  c("xlsx",
    "html",
    "r",
    "rmd")

prep_c_03 <-
  c("outputs",
    "outputs",
    "scripts",
    "scripts")

prep_d_01 <- 
  tibble(
    name = prep_c_02,
    value = prep_c_01,
    group = prep_c_03
  ) %>% 
  mutate(
    name = fct_reorder(name, value)
  )

prep_c_04 <-
  c(
    files_r %>%
      sapply(function(x)
        x %>% readLines() %>% length()) %>%
      sum(),
    files_rmd %>%
      sapply(function(x)
        x %>% readLines() %>% length()) %>%
      sum()
  )

prep_c_05 <-
  c("r",
    "rmd")

prep_d_02 <- 
  tibble(
    name = prep_c_05,
    value = prep_c_04,
  ) %>% 
  mutate(
    name = fct_reorder(name, value)
  )

issu_state <- 
  con %>%
  tbl(
    sql(
      glue::glue(
        "
      SELECT 
      sum(NetPaymentAmount) issuance
      , count(distinct casenumber) as cases
      FROM 
      MAXISStage.ESNAP_Covid19
      where 
TransactionDate >= date'{params$day_start}'
and TransactionDate <= date'{params$day_end}'
      "
      )
    )
  )  %>% 
  collect() %>% 
   mutate(issu_per_case = issuance/cases)
```

::: notes
As you may have guessed, I'm a quantitative data person. So I'd like to quantify all of this for you. The next few slides will give some data on the numbers of scripts, lines of code, number of reports and spreadsheets.
:::

## Scripts

```{r}
prep_d_01 %>% 
  filter(group == "scripts") %>% 
   mutate(
    name = fct_relevel(name,"r","rmd")
  ) %>% 
  ggplot(aes(x = name, y = value, fill = name)) +
  geom_col(stat = "identity") +
  geom_text(aes(y = .75*value, label = value), color = "white") +
  scale_fill_mn_state() +
  theme_fivethirtyeight() +
  gg_style() +
  theme(
    legend.position = "none"
  ) +
  labs(caption = "Source: MN DHS EAESD Research Team",
       title = "Dozens of r and rmd scripts")
```

::: notes
To get the process I've described running and to respond to adhoc questions, I wrote at least 41 r scripts and 28 rmd scripts. I say at least because this is what I see now and I'm not sure how many I deleted.
:::

## Lines of code

```{r}
prep_d_02 %>% 
  mutate(
    name = fct_relevel(name,"r","rmd")
  ) %>% 
  ggplot(aes(x = name, y = value, fill = name)) +
  geom_col(stat = "identity") +
  geom_text(aes(y = .75*value, label = scales::comma(value)), color = "white") +
  scale_fill_mn_state() +
  scale_y_continuous(labels = scales::comma) +
  theme_fivethirtyeight() +
  gg_style() +
  theme(
    legend.position = "none"
  ) +
  labs(caption = "Source: MN DHS EAESD Research Team",
       title = "Thousands of lines of r code")
```

::: notes
These r and rmd scripts amounted to a lot of code. There are more than 5000 lines of code in the r scripts and more than 16000 lines in the rmd files. As you may guess, the r scripts are considerable shorter. The r scripts were either functions, simple explorations, or scripts designed to execute rmd files. The rmd files composed the real work of the data extraction and processing. They were the core scripts that were executed monthly and produced the spreadsheets that the SNAP team needed to issue benefits.
:::

## Outputs

```{r}
prep_d_01 %>%
  filter(group == "outputs") %>%
  #  mutate(
  #   name = fct_relevel(name,"r","rmd")
  # ) %>%
  
  ggplot(aes(x = name, y = value, fill = name)) +
  geom_col(stat = "identity") +
  geom_text(aes(y = .75 * value, label = scales::comma(value)), color = "white") +
  scale_fill_mn_state() +
  scale_y_continuous(labels = scales::comma) +
  # theme_classic()
  theme_fivethirtyeight() +
  gg_style() +
  theme(
    legend.position = "none"
  ) +
  labs(caption = "Source: MN DHS EAESD Research Team",
       title = "Thousands of reports and spreadsheets")
```

::: notes
These scripts produced a lot of files. There were more than 1900 html files. Theses files were records of execution of the issuance scripts. And more than 9000 xlsx files. Remember each of these files needed to be either used in the MNIT script to issue benefits and used as a source to manually issuance. Our SNAP policy did so much work to get these benefits!
:::

## Issuance

```{epoxy}
#| echo: false

- As of {eaesdrrr::nice_month_year(params$day_end)}, Minnnesota Emergency SNAP issued:
  - **roughly {.dollar issu_state$issu_per_case} per case** to
  - **{scales::comma(issu_state$cases)} cases**, totalling
  - **{.dollar issu_state$issuance}**
```

::: notes
That's all well and good, but what does it matter to participants. As of October 2023, Minnesota Emergency SNAP issued more than \$3600 per case to more than 390000 cases. This amounts to more than \$1.4 billion dollars in food support to folks in Minnesota.
:::

## Analysis

Were there disparities in how Emergency SNAP was issued by race or ethnicity?

::: notes
That's awesome. We got a lot of food support out to families in Minnesota. But, I'm a researcher and start to get curious. Who got these benefits? Were there any racial disparities in the issuance of the benefits?
:::

## Was Emergency SNAP equitable?

```{r}
d_case <-
  readRDS(
    "C:/Users/pwbpj01/Documents/R_local/1257_SNAP_MFIP_Total_Issuance/data/report_data.rds"
  ) %>%
    filter(report_month < ymd('2023-03-01'))
  
  d_case <- 
  d_case %>% 
  mutate(
    race_white = fct_recode(race_white, BIPOC = 'Non-White')
  )

# Rename factor level

d_case <- 
  d_case %>% 
  mutate(
    ProgramID = factor(ProgramID),
    ProgramID = fct_recode(ProgramID, MFIP = 'MF', SNAP = 'FS')
  )

prep_2 <- 
  d_case %>% 
  filter(time > 1) %>%
  # filter(race_multiple_races == 'One race') %>%
  group_by(race_white) %>% 
  summarise(t = mean(issue_elig_difference,na.rm = TRUE)) %>% 
  mutate(name = 'Average increase per case') %>% 
  bind_rows(
    d_case %>% 
      filter(time > 1) %>%
      mutate(issue_elig_difference = issue_elig_difference/elig_persons) %>% 
      # filter(race_multiple_races == 'One race') %>%
      group_by(race_white) %>% 
      summarise(t = mean(issue_elig_difference,na.rm = TRUE)) %>% 
      mutate(name = 'Average increase per person')
    
  ) %>% 
  mutate(name = factor(name, levels = 
                         c('Average increase per person',
                           'Average increase per case')))

white_per_person <- 
  prep_2 %>% 
  filter(race_white == 'White') %>% 
  filter(name == 'Average increase per person') %>% 
  select(t) %>% 
  pull() %>% 
  scales::dollar()

BIPOC_per_person <- 
  prep_2 %>% 
  filter(race_white != 'White') %>% 
  filter(name == 'Average increase per person') %>% 
  select(t) %>% 
  pull() %>% 
  scales::dollar()

diff_per_person <- 
  abs(prep_2 %>% 
  filter(race_white == 'White') %>% 
  filter(name == 'Average increase per person') %>% 
  select(t) %>% 
  pull() - 
  prep_2 %>% 
  filter(race_white != 'White') %>% 
  filter(name == 'Average increase per person') %>% 
  select(t) %>% 
  pull()) %>% 
  scales::dollar()

white_per_case <- 
  prep_2 %>% 
  filter(race_white == 'White') %>% 
  filter(name == 'Average increase per case') %>% 
  select(t) %>% 
  pull() %>% 
  scales::dollar()

BIPOC_per_case <- 
  prep_2 %>% 
  filter(race_white != 'White') %>% 
  filter(name == 'Average increase per case') %>% 
  select(t) %>% 
  pull() %>% 
  scales::dollar()

diff_per_case <- 
  abs(prep_2 %>% 
  filter(race_white == 'White') %>% 
  filter(name == 'Average increase per case') %>% 
  select(t) %>% 
  pull() - 
  prep_2 %>% 
  filter(race_white != 'White') %>% 
  filter(name == 'Average increase per case') %>% 
  select(t) %>% 
  pull()) %>% 
  scales::dollar()

prep <- prep_2 %>% 
  mutate(
    name = fct_relabel(
      name,
      ~str_wrap(.,width = 15)
    )
  )

prep %>% 
  ggplot(aes(x= race_white, y = t, fill = race_white, label = scales::dollar(t))) +
  geom_bar(stat = 'identity', position = 'dodge') +
  geom_text(
    # size = params$chart.text.size,
    aes(y = t*.75), color = 'white') +
  facet_grid(name~., scales = 'free') +
  scale_fill_mn_state(palette = 'accent')+
  scale_y_continuous(labels = scales::dollar) +
  theme_fivethirtyeight() +
  gg_style() +
  theme(
    legend.position = 'none'
  ) +
  labs(caption = "Source: MN DHS EAESD Research Team",
       title = "Cases with white applicants received more per person in Emergency SNAP")
```

::: notes
While on average cases with BIPOC applicants received more per case in Emergency SNAP, at a per person level, these cases received much less per person than cases with white applicants. This difference persisted as we looked at whether case members were seniors, had disabilities, were families, had income. The difference remained. It's likely a results of the non-linear increase in benefits with case size. White cases are much more likely to be single person cases than BIPOC cases.
:::

## Data science in government

-   Nimble and responsive

-   Bridge between policy and IT

-   Extensions to analysis

::: notes
This has been my story of Minnesota's Emergency SNAP program. But I hope that it's also a motivating example of how data science can be be an asset in government programs. First, this example shows that data science can be nimble and responsive in a way that other areas may find troubling. Also, data science can be a bridge between policy and IT. Our IT systems are generally old and hard to adjust. Data science is build off data and code to do analysis, but it can do some more. Data scientists need to work closer to policy to answer questions that are useful. Policy workers generally don't have coding skills to solve some problems. But they tell me what the policies mean and what they need to implement them. Lastly, data science is generally build on quantitative research so for many of us there is a natural inclination to ask quantitative research questions.

Emergency SNAP was great program and one that I'm very proud to have worked on with my colleagues in MNIT and Policy.
:::

# Thank you!

Ben Jaques-Leslie (benjamin.jaques-leslie\@state.mn.us)
